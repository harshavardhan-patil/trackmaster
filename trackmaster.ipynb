{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tmrl\n",
      "  Downloading tmrl-0.7.1-py3-none-any.whl.metadata (2.7 kB)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from tmrl) (2.0.2)\n",
      "Requirement already satisfied: torch>=2.0 in /usr/local/lib/python3.12/dist-packages (from tmrl) (2.9.0+cu126)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from tmrl) (2.2.2)\n",
      "Requirement already satisfied: gymnasium in /usr/local/lib/python3.12/dist-packages (from tmrl) (1.2.2)\n",
      "Collecting rtgym>=0.13 (from tmrl)\n",
      "  Downloading rtgym-0.16-py3-none-any.whl.metadata (42 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.5/42.5 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: pyyaml in /usr/local/lib/python3.12/dist-packages (from tmrl) (6.0.3)\n",
      "Requirement already satisfied: wandb in /usr/local/lib/python3.12/dist-packages (from tmrl) (0.23.0)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from tmrl) (2.32.4)\n",
      "Requirement already satisfied: opencv-python in /usr/local/lib/python3.12/dist-packages (from tmrl) (4.12.0.88)\n",
      "Collecting pyautogui (from tmrl)\n",
      "  Downloading PyAutoGUI-0.9.54.tar.gz (61 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.2/61.2 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
      "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
      "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
      "Collecting pyinstrument (from tmrl)\n",
      "  Downloading pyinstrument-5.1.1-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (26 kB)\n",
      "Collecting tlspyo>=0.2.5 (from tmrl)\n",
      "  Downloading tlspyo-0.3.0.tar.gz (33 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "Requirement already satisfied: chardet in /usr/local/lib/python3.12/dist-packages (from tmrl) (5.2.0)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from tmrl) (25.0)\n",
      "INFO: pip is looking at multiple versions of tmrl to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting tmrl\n",
      "  Downloading tmrl-0.7.0.tar.gz (79 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.9/79.9 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
      "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
      "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
      "Collecting mss (from tmrl)\n",
      "  Downloading mss-10.1.0-py3-none-any.whl.metadata (6.7 kB)\n",
      "Collecting vgamepad>=0.1.0 (from tmrl)\n",
      "  Downloading vgamepad-0.1.0.tar.gz (1.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m77.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from gymnasium->tmrl) (3.1.2)\n",
      "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.12/dist-packages (from gymnasium->tmrl) (4.15.0)\n",
      "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.12/dist-packages (from gymnasium->tmrl) (0.0.4)\n",
      "Collecting twisted (from tlspyo>=0.2.5->tmrl)\n",
      "  Downloading twisted-25.5.0-py3-none-any.whl.metadata (22 kB)\n",
      "Requirement already satisfied: pyOpenSSL>22.1.0 in /usr/local/lib/python3.12/dist-packages (from tlspyo>=0.2.5->tmrl) (24.2.1)\n",
      "Collecting service_identity (from tlspyo>=0.2.5->tmrl)\n",
      "  Downloading service_identity-24.2.0-py3-none-any.whl.metadata (5.1 kB)\n",
      "Requirement already satisfied: platformdirs in /usr/local/lib/python3.12/dist-packages (from tlspyo>=0.2.5->tmrl) (4.5.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->tmrl) (3.20.0)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->tmrl) (75.2.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->tmrl) (1.14.0)\n",
      "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->tmrl) (3.5)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->tmrl) (3.1.6)\n",
      "Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->tmrl) (2025.3.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->tmrl) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->tmrl) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->tmrl) (12.6.80)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->tmrl) (9.10.2.21)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->tmrl) (12.6.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->tmrl) (11.3.0.4)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->tmrl) (10.3.7.77)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->tmrl) (11.7.1.2)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->tmrl) (12.5.4.2)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->tmrl) (0.7.1)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->tmrl) (2.27.5)\n",
      "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->tmrl) (3.3.20)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->tmrl) (12.6.77)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->tmrl) (12.6.85)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->tmrl) (1.11.1.6)\n",
      "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->tmrl) (3.5.0)\n",
      "Collecting libevdev~=0.11 (from vgamepad>=0.1.0->tmrl)\n",
      "  Downloading libevdev-0.13.1-py3-none-any.whl.metadata (2.2 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->tmrl) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->tmrl) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->tmrl) (2025.2)\n",
      "Collecting python3-Xlib (from pyautogui->tmrl)\n",
      "  Downloading python3-xlib-0.15.tar.gz (132 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m132.8/132.8 kB\u001b[0m \u001b[31m15.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "Collecting pymsgbox (from pyautogui->tmrl)\n",
      "  Downloading pymsgbox-2.0.1-py3-none-any.whl.metadata (4.4 kB)\n",
      "Collecting pytweening>=1.0.4 (from pyautogui->tmrl)\n",
      "  Downloading pytweening-1.2.0.tar.gz (171 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m171.2/171.2 kB\u001b[0m \u001b[31m19.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "Collecting pyscreeze>=0.1.21 (from pyautogui->tmrl)\n",
      "  Downloading pyscreeze-1.0.1.tar.gz (27 kB)\n",
      "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
      "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
      "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
      "Collecting pygetwindow>=0.0.5 (from pyautogui->tmrl)\n",
      "  Downloading PyGetWindow-0.0.9.tar.gz (9.7 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "Collecting mouseinfo (from pyautogui->tmrl)\n",
      "  Downloading MouseInfo-0.1.3.tar.gz (10 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->tmrl) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->tmrl) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->tmrl) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->tmrl) (2025.11.12)\n",
      "Requirement already satisfied: click>=8.0.1 in /usr/local/lib/python3.12/dist-packages (from wandb->tmrl) (8.3.1)\n",
      "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from wandb->tmrl) (3.1.45)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<7,>=3.19.0 in /usr/local/lib/python3.12/dist-packages (from wandb->tmrl) (5.29.5)\n",
      "Requirement already satisfied: pydantic<3 in /usr/local/lib/python3.12/dist-packages (from wandb->tmrl) (2.11.10)\n",
      "Requirement already satisfied: sentry-sdk>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from wandb->tmrl) (2.45.0)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.12/dist-packages (from gitpython!=3.1.29,>=1.0.0->wandb->tmrl) (4.0.12)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3->wandb->tmrl) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3->wandb->tmrl) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3->wandb->tmrl) (0.4.2)\n",
      "Collecting pyrect (from pygetwindow>=0.0.5->pyautogui->tmrl)\n",
      "  Downloading PyRect-0.2.0.tar.gz (17 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "Requirement already satisfied: cryptography<44,>=41.0.5 in /usr/local/lib/python3.12/dist-packages (from pyOpenSSL>22.1.0->tlspyo>=0.2.5->tmrl) (43.0.3)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas->tmrl) (1.17.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=2.0->tmrl) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=2.0->tmrl) (3.0.3)\n",
      "Requirement already satisfied: pyperclip in /usr/local/lib/python3.12/dist-packages (from mouseinfo->pyautogui->tmrl) (1.11.0)\n",
      "Requirement already satisfied: attrs>=19.1.0 in /usr/local/lib/python3.12/dist-packages (from service_identity->tlspyo>=0.2.5->tmrl) (25.4.0)\n",
      "Requirement already satisfied: pyasn1 in /usr/local/lib/python3.12/dist-packages (from service_identity->tlspyo>=0.2.5->tmrl) (0.6.1)\n",
      "Requirement already satisfied: pyasn1-modules in /usr/local/lib/python3.12/dist-packages (from service_identity->tlspyo>=0.2.5->tmrl) (0.4.2)\n",
      "Collecting automat>=24.8.0 (from twisted->tlspyo>=0.2.5->tmrl)\n",
      "  Downloading automat-25.4.16-py3-none-any.whl.metadata (8.4 kB)\n",
      "Collecting constantly>=15.1 (from twisted->tlspyo>=0.2.5->tmrl)\n",
      "  Downloading constantly-23.10.4-py3-none-any.whl.metadata (1.8 kB)\n",
      "Collecting hyperlink>=17.1.1 (from twisted->tlspyo>=0.2.5->tmrl)\n",
      "  Downloading hyperlink-21.0.0-py2.py3-none-any.whl.metadata (1.5 kB)\n",
      "Collecting incremental>=24.7.0 (from twisted->tlspyo>=0.2.5->tmrl)\n",
      "  Downloading incremental-24.7.2-py3-none-any.whl.metadata (8.1 kB)\n",
      "Collecting zope-interface>=5 (from twisted->tlspyo>=0.2.5->tmrl)\n",
      "  Downloading zope_interface-8.1.1-cp312-cp312-manylinux1_x86_64.manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_5_x86_64.whl.metadata (45 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.2/45.2 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.12/dist-packages (from cryptography<44,>=41.0.5->pyOpenSSL>22.1.0->tlspyo>=0.2.5->tmrl) (2.0.0)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.12/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb->tmrl) (5.0.2)\n",
      "Requirement already satisfied: pycparser in /usr/local/lib/python3.12/dist-packages (from cffi>=1.12->cryptography<44,>=41.0.5->pyOpenSSL>22.1.0->tlspyo>=0.2.5->tmrl) (2.23)\n",
      "Downloading rtgym-0.16-py3-none-any.whl (27 kB)\n",
      "Downloading mss-10.1.0-py3-none-any.whl (24 kB)\n",
      "Downloading pyinstrument-5.1.1-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (146 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m146.8/146.8 kB\u001b[0m \u001b[31m11.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading libevdev-0.13.1-py3-none-any.whl (33 kB)\n",
      "Downloading pymsgbox-2.0.1-py3-none-any.whl (10.0 kB)\n",
      "Downloading service_identity-24.2.0-py3-none-any.whl (11 kB)\n",
      "Downloading twisted-25.5.0-py3-none-any.whl (3.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m101.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading automat-25.4.16-py3-none-any.whl (42 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.8/42.8 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading constantly-23.10.4-py3-none-any.whl (13 kB)\n",
      "Downloading hyperlink-21.0.0-py2.py3-none-any.whl (74 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m74.6/74.6 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading incremental-24.7.2-py3-none-any.whl (20 kB)\n",
      "Downloading zope_interface-8.1.1-cp312-cp312-manylinux1_x86_64.manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_5_x86_64.whl (264 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m264.7/264.7 kB\u001b[0m \u001b[31m29.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hBuilding wheels for collected packages: tmrl, tlspyo, vgamepad, pyautogui, pygetwindow, pyscreeze, pytweening, mouseinfo, python3-Xlib, pyrect\n",
      "  Building wheel for tmrl (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for tmrl: filename=tmrl-0.7.0-py3-none-any.whl size=77133 sha256=88073ac316ee8beb13ef84e97c4e0e5363200db9954510253efdff86176208fb\n",
      "  Stored in directory: /root/.cache/pip/wheels/71/cf/e7/10927a3b9c84d5c826afcee43e1d8a5a2c98fd53310568b5da\n",
      "  Building wheel for tlspyo (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for tlspyo: filename=tlspyo-0.3.0-py3-none-any.whl size=27027 sha256=5d62620954977474b10c059f6401d2ebd5b6875b19fb552f03bbe3182ac65d71\n",
      "  Stored in directory: /root/.cache/pip/wheels/16/76/d2/6f82776baa10a3c231733256f9c180993c3d0b5a4f6188fd43\n",
      "  Building wheel for vgamepad (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for vgamepad: filename=vgamepad-0.1.0-py3-none-any.whl size=16253 sha256=f7b35353fa1fd646a25f6db80c6457f15f5b0ada1619b2df03bb7dc32ceb5524\n",
      "  Stored in directory: /root/.cache/pip/wheels/42/3c/09/f9eb2e2e91243e9beaca8e025c394d7416ecc9499897a5a236\n",
      "  Building wheel for pyautogui (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for pyautogui: filename=pyautogui-0.9.54-py3-none-any.whl size=37684 sha256=09b8ae9e9663b6b08ec0afed31e1113ac52b82c462f9a75221edfeae3dc6e390\n",
      "  Stored in directory: /root/.cache/pip/wheels/d9/d6/47/04075995b093ecc87c212c9a3dbd34e59456c6fe504d65c3e4\n",
      "  Building wheel for pygetwindow (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for pygetwindow: filename=PyGetWindow-0.0.9-py3-none-any.whl size=11063 sha256=bbdbd19e3e0d8e7cb917da4ddea4df388944ba27d0999bf61afe21ef329e84fb\n",
      "  Stored in directory: /root/.cache/pip/wheels/b3/39/81/34dd7a2eca5f885f1f6e2796761970daf66a2d98ac1904f5f4\n",
      "  Building wheel for pyscreeze (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for pyscreeze: filename=pyscreeze-1.0.1-py3-none-any.whl size=14459 sha256=fe5351bdf1f77866edb0143eb9dcabd7bd05ebb1d907771f70fa8fa993fdc422\n",
      "  Stored in directory: /root/.cache/pip/wheels/cd/3a/c2/7f2839239a069aa3c9564f6777cbb29d733720ef673f104f0d\n",
      "  Building wheel for pytweening (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for pytweening: filename=pytweening-1.2.0-py3-none-any.whl size=8009 sha256=05b3c15a59d59e60c3ee2fcdcf18a5d45ef42bd00d62f40d29d422a6fbf6aa07\n",
      "  Stored in directory: /root/.cache/pip/wheels/23/d5/13/4e9bdadbfe3c78e47c675e7410c0eed2fbb63c5ea6cf1b40e7\n",
      "  Building wheel for mouseinfo (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for mouseinfo: filename=MouseInfo-0.1.3-py3-none-any.whl size=10888 sha256=44dba14621df0208e72a4a7d39407897c2e19a80156b013eb8fc6ec9db6544f2\n",
      "  Stored in directory: /root/.cache/pip/wheels/b1/9b/f3/08650eb7f00af32f07789f3c6a101e0d7fc762b9891ae843bb\n",
      "  Building wheel for python3-Xlib (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for python3-Xlib: filename=python3_xlib-0.15-py3-none-any.whl size=109501 sha256=8dc7978efd8fa2619f05a04f4cb466c651305455b37ff87f981218577450a4ca\n",
      "  Stored in directory: /root/.cache/pip/wheels/a1/4c/74/f3627dc31a082129210025befb30084e73af761d007ac5c7b3\n",
      "  Building wheel for pyrect (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for pyrect: filename=PyRect-0.2.0-py2.py3-none-any.whl size=11181 sha256=9f528ad03db7f3ddff7c701f42e707bfbf1d673a3545dedfc595c51b988325d8\n",
      "  Stored in directory: /root/.cache/pip/wheels/0b/1e/d7/0c74bd8f60b39c14d84e307398786002aa7ddc905927cc03c5\n",
      "Successfully built tmrl tlspyo vgamepad pyautogui pygetwindow pyscreeze pytweening mouseinfo python3-Xlib pyrect\n",
      "Installing collected packages: pytweening, python3-Xlib, pyscreeze, pyrect, zope-interface, pymsgbox, pyinstrument, pygetwindow, mss, mouseinfo, libevdev, incremental, hyperlink, constantly, automat, vgamepad, twisted, rtgym, pyautogui, service_identity, tlspyo, tmrl\n",
      "Successfully installed automat-25.4.16 constantly-23.10.4 hyperlink-21.0.0 incremental-24.7.2 libevdev-0.13.1 mouseinfo-0.1.3 mss-10.1.0 pyautogui-0.9.54 pygetwindow-0.0.9 pyinstrument-5.1.1 pymsgbox-2.0.1 pyrect-0.2.0 pyscreeze-1.0.1 python3-Xlib-0.15 pytweening-1.2.0 rtgym-0.16 service_identity-24.2.0 tlspyo-0.3.0 tmrl-0.7.0 twisted-25.5.0 vgamepad-0.1.0 zope-interface-8.1.1\n"
     ]
    }
   ],
   "source": [
    "%pip install tmrl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tmrl\n",
    "import time\n",
    "import matplotlib.pyplot as plts\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'Tesla T4'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "torch.cuda.get_device_name(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Observation Space:\t Tuple(Box(0.0, 1000.0, (1,), float32), Box(0.0, 6.0, (1,), float32), Box(0.0, inf, (1,), float32), Box(0.0, 255.0, (4, 64, 64), float32), Box(-1.0, 1.0, (3,), float32), Box(-1.0, 1.0, (3,), float32))\n",
      "Action Space:\t\t Box(-1.0, 1.0, (3,), float32)\n",
      "observation_space logits: 16393\n",
      "action_space logits:\t 3\n"
     ]
    }
   ],
   "source": [
    "env = tmrl.get_environment()\n",
    "print('Observation Space:\\t', env.observation_space)\n",
    "print('Action Space:\\t\\t', env.action_space)\n",
    "observation_space = np.sum([np.prod(value.shape) for value in env.observation_space])\n",
    "action_space = env.action_space.shape[0]\n",
    "print('observation_space logits:', observation_space)\n",
    "print('action_space logits:\\t', action_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyper_params = {'policy_lr': 1e-5,\n",
    "                'critic_lr': 1e-5,\n",
    "                'gamma': 0.996,\n",
    "                'clip_coef': 0.2,\n",
    "                'critic_coef': 0.1,\n",
    "                'entropy_coef': 0.1,\n",
    "                'batch_size': 256,\n",
    "                'num_updates': 10000,\n",
    "                'epochs_per_update': 100,\n",
    "                'hidden_dim':512,\n",
    "                'max_episode_steps': 2400,\n",
    "                'norm_advantages': True,\n",
    "                'grad_clip_val': 0.1,\n",
    "                'initial_std': 1,\n",
    "                'avg_ray': 400}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Policy (nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.action_mean = nn.Sequential(\n",
    "            #nn.LayerNorm(observation_space),\n",
    "            #nn.BatchNorm1d(observation_space),\n",
    "            nn.Linear(observation_space, hyper_params['hidden_dim']),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hyper_params['hidden_dim'],hyper_params['hidden_dim']),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hyper_params['hidden_dim'], action_space),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "        self.actor_logvar = nn.Sequential(\n",
    "            #nn.LayerNorm(observation_space),\n",
    "            #nn.BatchNorm1d(observation_space),\n",
    "            nn.Linear(observation_space, hyper_params['hidden_dim']),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hyper_params['hidden_dim'],hyper_params['hidden_dim']),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hyper_params['hidden_dim'], 1)\n",
    "        )\n",
    "\n",
    "    def sample_action_with_logprobs(self, observation):\n",
    "        dist = self(observation)\n",
    "        sample_action = dist.sample()\n",
    "        return sample_action, dist.log_prob(sample_action)\n",
    "    \n",
    "    def mean_only(self, observation):\n",
    "        with torch.no_grad():\n",
    "            return self.action_mean(observation)\n",
    "    \n",
    "    def get_action_log_prob(self, observation, action):\n",
    "        dist = self(observation)\n",
    "        return dist.log_prob(action)\n",
    "    \n",
    "    def forward(self, observation):\n",
    "        observation /= hyper_params['avg_ray']\n",
    "        means = self.action_mean(observation)\n",
    "        vars = torch.zeros(observation.shape[0], action_space).to(device)\n",
    "        vars[:,:] = self.actor_logvar(observation).exp().view(-1,1)\n",
    "        covar_mat = torch.zeros(observation.shape[0], action_space, action_space).to(device)\n",
    "        covar_mat[:,np.arange(action_space), np.arange(action_space)] = vars\n",
    "\n",
    "        dist = torch.distributions.MultivariateNormal(means, covar_mat)\n",
    "        return dist\n",
    "        \n",
    "class Critic (nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.network = nn.Sequential(\n",
    "            #nn.LayerNorm(observation_space),\n",
    "            #nn.BatchNorm1d(observation_space),\n",
    "            nn.Linear(observation_space, hyper_params['hidden_dim']),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hyper_params['hidden_dim'],hyper_params['hidden_dim']),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hyper_params['hidden_dim'], 1)\n",
    "        )\n",
    "    def forward(self, observation):\n",
    "        observation /= hyper_params['avg_ray']\n",
    "        return self.network(observation)\n",
    "    \n",
    "class Agent(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.policy = Policy()\n",
    "        self.critic = Critic()\n",
    "    def forward(self, x):\n",
    "        raise SyntaxError('Propagate through Agent.policy \\\n",
    "                          and Agent.critic individually')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def env_obs_to_tensor(observations):\n",
    "    tensors = [torch.tensor(observation).view(-1) for observation in observations]\n",
    "    return torch.cat(tuple(tensors), dim=-1)\n",
    "\n",
    "def env_act_to_tensor(action):\n",
    "    return torch.tensor(action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = Agent().to(device)\n",
    "policy_optim = torch.optim.Adam(agent.policy.parameters(), lr=hyper_params['policy_lr'])\n",
    "critic_optim = torch.optim.Adam(agent.critic.parameters(), lr=hyper_params['critic_lr'])\n",
    "#agent.load_state_dict(torch.load('130.33999633789062RewardRacer56Update.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def train_PPO():\n    \"\"\"\n    Main PPO training loop that collects experience, computes advantages, and optimizes policy.\n    \n    PPO (Proximal Policy Optimization) is an on-policy algorithm that:\n    1. Collects trajectories using the current policy\n    2. Computes advantages (how much better actions were than expected)\n    3. Updates policy using clipped surrogate objective to prevent large updates\n    4. Updates value function to better estimate returns\n    \"\"\"\n    # Lists to track training progress over all updates\n    cum_rewards = []\n    actor_losses = []\n    critic_losses = []\n    total_losses = []\n\n    cum_reward = 0\n    \n    # Main training loop - each update is one complete episode\n    for update in range(hyper_params['num_updates']):        \n        \n        # ==================== PHASE 1: DATA COLLECTION (ROLLOUT) ====================\n        # Buffers to store experience from one complete episode\n        obs = torch.zeros(hyper_params['max_episode_steps'], observation_space)\n        actions = torch.zeros(hyper_params['max_episode_steps'], action_space)\n        logprobs = torch.zeros(hyper_params['max_episode_steps'])  # Log probability of each action\n        rewards = torch.zeros(hyper_params['max_episode_steps'])\n        state_values = torch.zeros(hyper_params['max_episode_steps'])  # Critic's value estimates\n        returns = torch.zeros(hyper_params['max_episode_steps'])  # Will store actual cumulative rewards\n        \n        # TODO: Add learning rate scheduler if needed\n        \n        # Reset the environment before each new episode\n        next_obs = env_obs_to_tensor(env.reset()[0])  # Extract observation from (obs, info) tuple\n        \n        # TMRL-specific: Manually click game window to ensure focus when training\n        # Uncomment if needed for first update\n        #if update == 0:\n            #time.sleep(1.0)\n        \n        max_idx = 0  # Track actual episode length (may be less than max_episode_steps)\n        was_terminated = False  # Track if episode ended naturally vs. truncated\n        \n        # Set agent to evaluation mode (no gradient tracking during rollout)\n        agent.eval()\n        \n        # Collect experience for one complete episode\n        for step in range(hyper_params['max_episode_steps']):\n            obs[step] = next_obs\n\n            # Sample action from current policy without gradient tracking\n            with torch.no_grad():\n                # Get action and its log probability from stochastic policy\n                action, logprob = agent.policy.sample_action_with_logprobs(next_obs.to(device).unsqueeze(0))\n                # Get critic's estimate of state value (for advantage calculation later)\n                state_value = agent.critic(next_obs.to(device).unsqueeze(0))\n                \n            # Store action, log probability, and value estimate\n            actions[step] = action[0]\n            logprobs[step] = logprob[0]\n            state_values[step] = state_value[0]\n\n            # Clip actions to valid range since Gaussian sampling can produce out-of-bounds values\n            clamped_action = np.clip(np.array(action.cpu()), -1, 1)\n\n            # Execute action in environment\n            next_obs, reward, terminated, truncated, info = env.step(clamped_action[0])\n\n            # Custom termination condition: End episode if car gets stuck on rail\n            # (detected by low LIDAR readings)\n            if next_obs[2][next_obs[2] <= 40].sum() > 0:\n                terminated = True\n            \n            # Store reward and convert next observation to tensor\n            rewards[step] = torch.tensor(reward)\n            next_obs = env_obs_to_tensor(next_obs)\n            \n            # Break if episode ended\n            if terminated or truncated:\n                was_terminated = True\n                max_idx = step\n                break\n                \n        # Pause environment (TMRL-specific requirement)\n        env.unwrapped.wait() # type: ignore\n        \n        max_idx = step  # Store final step index\n        \n        # ==================== PHASE 2: COMPUTE RETURNS & ADVANTAGES ====================\n        # Calculate discounted cumulative rewards (returns) working backwards from end\n        with torch.no_grad():\n            for t in range(max_idx + 1)[::-1]:  # Iterate backwards from last step\n                if t == (max_idx):\n                    if not was_terminated:\n                        # If episode was truncated (not naturally ended), bootstrap with critic\n                        # Currently disabled - only using immediate reward\n                        returns[t] = rewards[t]  # + (hyper_params['gamma']*agent.critic(next_obs.to(device)))\n                    else:\n                        # Terminal state has no future rewards\n                        returns[t] = rewards[t]\n                else:\n                    # Standard discounted return: R_t = r_t + gamma * R_{t+1}\n                    returns[t] = rewards[t] + (hyper_params['gamma'] * returns[t+1])\n                    \n            # Compute advantages: A = actual_return - predicted_value\n            # Positive advantage means action was better than expected\n            advantages = returns - state_values\n            cum_reward = rewards.sum().item()\n            \n        # Save model checkpoint if performance is good (reward > 200)\n        if cum_reward > 200:\n            torch.save(agent.state_dict(), f'Y{cum_reward:.2f}RewardRacer{update}Update_2.pt')\n        \n        # ==================== PHASE 3: POLICY OPTIMIZATION ====================\n        # Randomly shuffle indices for mini-batch training\n        rand_idxs = np.random.permutation(np.arange(max_idx + 1))\n        \n        # Track losses for each epoch\n        epochs_values_loss = []\n        epochs_ppo_loss = []\n        epochs_total_loss = []\n        \n        # Set agent to training mode (enable gradient tracking)\n        agent.train()\n        \n        # Train on collected data for multiple epochs\n        for epoch in range(hyper_params['epochs_per_update']):\n            # Process data in mini-batches\n            for batch_start_idx in range(0, max_idx, hyper_params['batch_size']):\n                batch_end_idx = batch_start_idx + hyper_params['batch_size']\n                batch_idxs = rand_idxs[batch_start_idx:batch_end_idx]\n\n                # Extract batch data\n                batch_obs = obs[batch_idxs]\n                batch_actions = actions[batch_idxs]\n\n                # ========== PPO CLIPPED SURROGATE OBJECTIVE ==========\n                # Get new log probabilities under current (updated) policy\n                batch_new_log_probs = agent.policy.get_action_log_prob(batch_obs.to(device), batch_actions.to(device))\n                # Get old log probabilities from when actions were sampled\n                batch_old_log_probs = logprobs[batch_idxs].to(device)\n\n                # Compute importance sampling ratio: pi_new(a|s) / pi_old(a|s)\n                log_ratio = batch_new_log_probs - batch_old_log_probs\n                ratio = log_ratio.exp()\n\n                # Normalize advantages to reduce variance (optional but recommended)\n                batch_advantages = advantages[batch_idxs].to(device)\n                if hyper_params['norm_advantages']:\n                    batch_advantages = (batch_advantages - batch_advantages.mean()) / (batch_advantages.std() + 1e-8)\n\n                # PPO clipped objective:\n                # - Unclipped: L = ratio * advantage\n                # - Clipped: L = clip(ratio, 1-ε, 1+ε) * advantage\n                # Take the minimum (most conservative) to prevent large policy updates\n                unclipped_obj = -ratio * batch_advantages\n                clipped_obj = -torch.clip(ratio, 1 - hyper_params['clip_coef'], 1 + hyper_params['clip_coef']) * batch_advantages\n                ppo_loss = torch.max(unclipped_obj, clipped_obj).sum() / hyper_params['batch_size']\n                epochs_ppo_loss.append(ppo_loss.item())\n\n                # ========== VALUE FUNCTION LOSS ==========\n                # Train critic to predict returns using MSE loss\n                new_state_values = agent.critic(batch_obs.to(device))\n                v_loss = ((new_state_values.view(-1) - returns[batch_idxs].to(device))**2).sum() / hyper_params['batch_size']\n                epochs_values_loss.append(v_loss.item())\n            \n                # Combined loss: policy loss + value loss (weighted)\n                total_loss = ppo_loss + hyper_params['critic_coef'] * v_loss\n                epochs_total_loss.append(total_loss.item())\n                \n                # ========== GRADIENT DESCENT ==========\n                # Zero gradients\n                policy_optim.zero_grad()\n                critic_optim.zero_grad()\n                \n                # Backpropagate\n                total_loss.backward()\n\n                # Clip policy gradients to prevent exploding gradients\n                nn.utils.clip_grad.clip_grad_value_(agent.policy.parameters(), clip_value=hyper_params['grad_clip_val'])\n                # Handle NaN gradients by setting them to zero\n                for param in agent.policy.parameters():\n                    mask = torch.isnan(param.grad)\n                    param.grad[mask] = 0.0\n                    if mask.sum == param.numel():\n                        print('code is broken, yup')  # All gradients are NaN - major issue\n                # Update policy parameters\n                policy_optim.step()\n\n                # Clip critic gradients and update\n                nn.utils.clip_grad.clip_grad_value_(agent.critic.parameters(), clip_value=hyper_params['grad_clip_val'])\n                critic_optim.step()\n                \n        # ==================== LOGGING ====================\n        # Print progress for this update\n        print('Update', update + 1)\n        print('actor loss', np.mean(epochs_ppo_loss))\n        print('critic loss', np.mean(epochs_values_loss))\n        print('total loss', np.mean(epochs_total_loss))\n        print('total reward', cum_reward)\n        \n        # Track metrics for plotting\n        cum_rewards.append(cum_reward)\n        actor_losses.append(np.mean(epochs_ppo_loss))\n        critic_losses.append(np.mean(epochs_values_loss))\n        total_losses.append(np.mean(epochs_total_loss))\n        \n    return cum_rewards, actor_losses, critic_losses, total_losses"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread Thread-12 (__background_thread):\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.12/threading.py\", line 1075, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.12/threading.py\", line 1012, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/rtgym/envs/real_time_env.py\", line 763, in __background_thread\n",
      "    self.__reset_result = self.__interface.reset(seed=seed, options=options)\n",
      "                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/tmrl/custom/tm/tm_gym_interfaces.py\", line 147, in reset\n",
      "    self.reset_common()\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/tmrl/custom/tm/tm_gym_interfaces.py\", line 137, in reset_common\n",
      "    self.initialize()\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/tmrl/custom/tm/tm_gym_interfaces.py\", line 89, in initialize\n",
      "    self.initialize_common()\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/tmrl/custom/tm/tm_gym_interfaces.py\", line 72, in initialize_common\n",
      "    import vgamepad as vg\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/vgamepad/__init__.py\", line 7, in <module>\n",
      "    from vgamepad.lin.virtual_gamepad import VX360Gamepad, VDS4Gamepad\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/vgamepad/lin/virtual_gamepad.py\", line 7, in <module>\n",
      "    import libevdev\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/libevdev/__init__.py\", line 22, in <module>\n",
      "    from .device import Device, InputAbsInfo\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/libevdev/device.py\", line 37, in <module>\n",
      "    from .event import InputEvent\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/libevdev/event.py\", line 24, in <module>\n",
      "    from .const import EventType, EventCode\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/libevdev/const.py\", line 554, in <module>\n",
      "    _load_consts()\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/libevdev/const.py\", line 482, in _load_consts\n",
      "    Libevdev()  # classmethods, need to make sure it's loaded at once\n",
      "    ^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/libevdev/_clib.py\", line 407, in __init__\n",
      "    super().__init__()\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/libevdev/_clib.py\", line 89, in __init__\n",
      "    self._load()\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/libevdev/_clib.py\", line 96, in _load\n",
      "    cls._lib = cls._cdll()\n",
      "               ^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/libevdev/_clib.py\", line 136, in _cdll\n",
      "    return ctypes.CDLL(\"libevdev.so.2\", use_errno=True)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/lib/python3.12/ctypes/__init__.py\", line 379, in __init__\n",
      "    self._handle = _dlopen(self._name, mode)\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "OSError: libevdev.so.2: cannot open shared object file: No such file or directory\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipython-input-2794494971.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/gymnasium/core.py\u001b[0m in \u001b[0;36mreset\u001b[0;34m(self, seed, options)\u001b[0m\n\u001b[1;32m    331\u001b[0m     ) -> tuple[WrapperObsType, dict[str, Any]]:\n\u001b[1;32m    332\u001b[0m         \u001b[0;34m\"\"\"Uses the :meth:`reset` of the :attr:`env` that can be overwritten to change the returned data.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 333\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    334\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    335\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mRenderFrame\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mRenderFrame\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/gymnasium/wrappers/common.py\u001b[0m in \u001b[0;36mreset\u001b[0;34m(self, seed, options)\u001b[0m\n\u001b[1;32m    398\u001b[0m         \u001b[0;34m\"\"\"Resets the environment with `kwargs`.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    399\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_has_reset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 400\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    401\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    402\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mRenderFrame\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mRenderFrame\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/gymnasium/core.py\u001b[0m in \u001b[0;36mreset\u001b[0;34m(self, seed, options)\u001b[0m\n\u001b[1;32m    331\u001b[0m     ) -> tuple[WrapperObsType, dict[str, Any]]:\n\u001b[1;32m    332\u001b[0m         \u001b[0;34m\"\"\"Uses the :meth:`reset` of the :attr:`env` that can be overwritten to change the returned data.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 333\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    334\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    335\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mRenderFrame\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mRenderFrame\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/rtgym/envs/real_time_env.py\u001b[0m in \u001b[0;36mreset\u001b[0;34m(self, seed, options)\u001b[0m\n\u001b[1;32m    831\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__ts_running\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    832\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__ev_start_ts\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 833\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__ev_end_reset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    834\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__ev_end_reset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    835\u001b[0m         \u001b[0melt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__reset_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.12/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    653\u001b[0m             \u001b[0msignaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_flag\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    654\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msignaled\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 655\u001b[0;31m                 \u001b[0msignaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cond\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    656\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0msignaled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    657\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.12/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    353\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m    \u001b[0;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    354\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 355\u001b[0;31m                 \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    356\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    357\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "time.sleep(1)\n",
    "env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "g:\\projects\\trackmaster\\.venv\\lib\\site-packages\\rtgym\\envs\\real_time_env.py:261: UserWarning: Time-step timed out. Elapsed since last time-step: 6.73626349999995\n",
      "  warnings.warn(f\"Time-step timed out. Elapsed since last time-step: {now - self.__t_end}\")\n",
      "C:\\Users\\Harshavardhan Patil\\AppData\\Local\\Temp\\ipykernel_11440\\2156043525.py:43: DeprecationWarning: __array__ implementation doesn't accept a copy keyword, so passing copy=False failed. __array__ must implement 'dtype' and 'copy' keyword arguments. To learn more, see the migration guide https://numpy.org/devdocs/numpy_2_0_migration_guide.html#adapting-to-changes-in-the-copy-keyword\n",
      "  clamped_action = np.clip(np.array(action.cpu()),-1,1)\n",
      "g:\\projects\\trackmaster\\.venv\\lib\\site-packages\\rtgym\\envs\\real_time_env.py:261: UserWarning: Time-step timed out. Elapsed since last time-step: 0.24284929999976157\n",
      "  warnings.warn(f\"Time-step timed out. Elapsed since last time-step: {now - self.__t_end}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Update 1\n",
      "actor loss nan\n",
      "critic loss nan\n",
      "total loss nan\n",
      "total reward 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "g:\\projects\\trackmaster\\.venv\\lib\\site-packages\\numpy\\_core\\fromnumeric.py:3860: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "g:\\projects\\trackmaster\\.venv\\lib\\site-packages\\numpy\\_core\\_methods.py:145: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    }
   ],
   "source": [
    "cum_rewards, actor_losses, critic_losses, total_losses = train_PPO()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Expected parameter covariance_matrix (Tensor of shape (3, 3)) of distribution MultivariateNormal(loc: torch.Size([3]), covariance_matrix: torch.Size([3, 3])) to satisfy the constraint PositiveDefinite(), but found invalid values:\ntensor([[ 0.4632, 10.1557,  0.2630],\n        [ 2.6142,  1.1253,  3.7585],\n        [ 0.4767,  0.2665,  0.3334]])",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdistributions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mMultivariateNormal\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mzeros\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrandn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m9\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreshape\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexp\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mg:\\projects\\trackmaster\\.venv\\lib\\site-packages\\torch\\distributions\\multivariate_normal.py:184\u001b[0m, in \u001b[0;36mMultivariateNormal.__init__\u001b[1;34m(self, loc, covariance_matrix, precision_matrix, scale_tril, validate_args)\u001b[0m\n\u001b[0;32m    181\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloc \u001b[38;5;241m=\u001b[39m loc\u001b[38;5;241m.\u001b[39mexpand(batch_shape \u001b[38;5;241m+\u001b[39m (\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m,))\n\u001b[0;32m    183\u001b[0m event_shape \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloc\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m:]\n\u001b[1;32m--> 184\u001b[0m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mbatch_shape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mevent_shape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidate_args\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidate_args\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    186\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m scale_tril \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    187\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_unbroadcasted_scale_tril \u001b[38;5;241m=\u001b[39m scale_tril\n",
      "File \u001b[1;32mg:\\projects\\trackmaster\\.venv\\lib\\site-packages\\torch\\distributions\\distribution.py:77\u001b[0m, in \u001b[0;36mDistribution.__init__\u001b[1;34m(self, batch_shape, event_shape, validate_args)\u001b[0m\n\u001b[0;32m     75\u001b[0m         valid \u001b[38;5;241m=\u001b[39m constraint\u001b[38;5;241m.\u001b[39mcheck(value)\n\u001b[0;32m     76\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_is_all_true(valid):\n\u001b[1;32m---> 77\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m     78\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected parameter \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mparam\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     79\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(value)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m of shape \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtuple\u001b[39m(value\u001b[38;5;241m.\u001b[39mshape)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     80\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mof distribution \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mrepr\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     81\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mto satisfy the constraint \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mrepr\u001b[39m(constraint)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     82\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbut found invalid values:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mvalue\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     83\u001b[0m             )\n\u001b[0;32m     84\u001b[0m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m()\n",
      "\u001b[1;31mValueError\u001b[0m: Expected parameter covariance_matrix (Tensor of shape (3, 3)) of distribution MultivariateNormal(loc: torch.Size([3]), covariance_matrix: torch.Size([3, 3])) to satisfy the constraint PositiveDefinite(), but found invalid values:\ntensor([[ 0.4632, 10.1557,  0.2630],\n        [ 2.6142,  1.1253,  3.7585],\n        [ 0.4767,  0.2665,  0.3334]])"
     ]
    }
   ],
   "source": [
    "torch.distributions.MultivariateNormal(torch.zeros(3), torch.randn(9).reshape(3,3).exp())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, env):\n",
    "    done = False\n",
    "    time.sleep(1.0)\n",
    "    next_state = env_obs_to_tensor(env.reset()[0]).to(device)\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    while not done:\n",
    "        action, logprob = model.policy.sample_action_with_logprobs(next_state.to(device).unsqueeze(0))\n",
    "        #action = model.policy.mean_only(next_state.to(device).unsqueeze(0))\n",
    "        clamped_action = np.clip((action.detach().cpu().numpy())[0], -1,1)\n",
    "        next_state, reward, done, truncated, info = env.step(clamped_action)\n",
    "        next_state = env_obs_to_tensor(next_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for Agent:\n\tsize mismatch for policy.action_mean.0.weight: copying a param with shape torch.Size([512, 84]) from checkpoint, the shape in current model is torch.Size([512, 602121]).\n\tsize mismatch for policy.actor_logvar.0.weight: copying a param with shape torch.Size([512, 84]) from checkpoint, the shape in current model is torch.Size([512, 602121]).\n\tsize mismatch for critic.network.0.weight: copying a param with shape torch.Size([512, 84]) from checkpoint, the shape in current model is torch.Size([512, 602121]).",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m agent \u001b[38;5;241m=\u001b[39m Agent()\n\u001b[1;32m----> 2\u001b[0m \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_state_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mY231.38RewardRacer164Update_2.pt\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mg:\\projects\\trackmaster\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:2629\u001b[0m, in \u001b[0;36mModule.load_state_dict\u001b[1;34m(self, state_dict, strict, assign)\u001b[0m\n\u001b[0;32m   2621\u001b[0m         error_msgs\u001b[38;5;241m.\u001b[39minsert(\n\u001b[0;32m   2622\u001b[0m             \u001b[38;5;241m0\u001b[39m,\n\u001b[0;32m   2623\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMissing key(s) in state_dict: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m   2624\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m missing_keys)\n\u001b[0;32m   2625\u001b[0m             ),\n\u001b[0;32m   2626\u001b[0m         )\n\u001b[0;32m   2628\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(error_msgs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m-> 2629\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m   2630\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError(s) in loading state_dict for \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m   2631\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(error_msgs)\n\u001b[0;32m   2632\u001b[0m         )\n\u001b[0;32m   2633\u001b[0m     )\n\u001b[0;32m   2634\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for Agent:\n\tsize mismatch for policy.action_mean.0.weight: copying a param with shape torch.Size([512, 84]) from checkpoint, the shape in current model is torch.Size([512, 602121]).\n\tsize mismatch for policy.actor_logvar.0.weight: copying a param with shape torch.Size([512, 84]) from checkpoint, the shape in current model is torch.Size([512, 602121]).\n\tsize mismatch for critic.network.0.weight: copying a param with shape torch.Size([512, 84]) from checkpoint, the shape in current model is torch.Size([512, 602121])."
     ]
    }
   ],
   "source": [
    "agent = Agent()\n",
    "agent.load_state_dict(torch.load('Y231.38RewardRacer164Update_2.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\xande\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\rtgym\\envs\\real_time_env.py:376: UserWarning: Time-step timed out. Elapsed since last time-step: 13.214103900001646\n",
      "  warnings.warn(f\"Time-step timed out. Elapsed since last time-step: {now - self.__t_end}\")\n"
     ]
    }
   ],
   "source": [
    "env.reset()\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\xande\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\rtgym\\envs\\real_time_env.py:376: UserWarning: Time-step timed out. Elapsed since last time-step: 3.6319978999999876\n",
      "  warnings.warn(f\"Time-step timed out. Elapsed since last time-step: {now - self.__t_end}\")\n",
      "C:\\Users\\xande\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\rtgym\\envs\\real_time_env.py:376: UserWarning: Time-step timed out. Elapsed since last time-step: 6.322334800001045\n",
      "  warnings.warn(f\"Time-step timed out. Elapsed since last time-step: {now - self.__t_end}\")\n",
      "C:\\Users\\xande\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\rtgym\\envs\\real_time_env.py:376: UserWarning: Time-step timed out. Elapsed since last time-step: 0.051987200507937814\n",
      "  warnings.warn(f\"Time-step timed out. Elapsed since last time-step: {now - self.__t_end}\")\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m evaluate_model(agent, env)\n",
      "Cell \u001b[1;32mIn[12], line 11\u001b[0m, in \u001b[0;36mevaluate_model\u001b[1;34m(model, env)\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[39m#action = model.policy.mean_only(next_state.to(device).unsqueeze(0))\u001b[39;00m\n\u001b[0;32m     10\u001b[0m clamped_action \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mclip((action\u001b[39m.\u001b[39mdetach()\u001b[39m.\u001b[39mcpu()\u001b[39m.\u001b[39mnumpy())[\u001b[39m0\u001b[39m], \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m,\u001b[39m1\u001b[39m)\n\u001b[1;32m---> 11\u001b[0m next_state, reward, done, truncated, info \u001b[39m=\u001b[39m env\u001b[39m.\u001b[39;49mstep(clamped_action)\n\u001b[0;32m     12\u001b[0m next_state \u001b[39m=\u001b[39m env_obs_to_tensor(next_state)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\gymnasium\\core.py:408\u001b[0m, in \u001b[0;36mWrapper.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m    404\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mstep\u001b[39m(\n\u001b[0;32m    405\u001b[0m     \u001b[39mself\u001b[39m, action: WrapperActType\n\u001b[0;32m    406\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mtuple\u001b[39m[WrapperObsType, SupportsFloat, \u001b[39mbool\u001b[39m, \u001b[39mbool\u001b[39m, \u001b[39mdict\u001b[39m[\u001b[39mstr\u001b[39m, Any]]:\n\u001b[0;32m    407\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Uses the :meth:`step` of the :attr:`env` that can be overwritten to change the returned data.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 408\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menv\u001b[39m.\u001b[39;49mstep(action)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\tmrl\\wrappers.py:33\u001b[0m, in \u001b[0;36mFloat64ToFloat32.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mstep\u001b[39m(\u001b[39mself\u001b[39m, action):\n\u001b[1;32m---> 33\u001b[0m     s, r, d, t, info \u001b[39m=\u001b[39m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mstep(action)\n\u001b[0;32m     34\u001b[0m     \u001b[39mreturn\u001b[39;00m s, r, d, t, info\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\gymnasium\\core.py:469\u001b[0m, in \u001b[0;36mObservationWrapper.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m    465\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mstep\u001b[39m(\n\u001b[0;32m    466\u001b[0m     \u001b[39mself\u001b[39m, action: ActType\n\u001b[0;32m    467\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mtuple\u001b[39m[WrapperObsType, SupportsFloat, \u001b[39mbool\u001b[39m, \u001b[39mbool\u001b[39m, \u001b[39mdict\u001b[39m[\u001b[39mstr\u001b[39m, Any]]:\n\u001b[0;32m    468\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Modifies the :attr:`env` after calling :meth:`step` using :meth:`self.observation` on the returned observations.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 469\u001b[0m     observation, reward, terminated, truncated, info \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menv\u001b[39m.\u001b[39;49mstep(action)\n\u001b[0;32m    470\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mobservation(observation), reward, terminated, truncated, info\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\gymnasium\\wrappers\\order_enforcing.py:56\u001b[0m, in \u001b[0;36mOrderEnforcing.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     54\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_has_reset:\n\u001b[0;32m     55\u001b[0m     \u001b[39mraise\u001b[39;00m ResetNeeded(\u001b[39m\"\u001b[39m\u001b[39mCannot call env.step() before calling env.reset()\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m---> 56\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menv\u001b[39m.\u001b[39;49mstep(action)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\rtgym\\envs\\real_time_env.py:545\u001b[0m, in \u001b[0;36mRealTimeEnv.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m    543\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbenchmark:\n\u001b[0;32m    544\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbench\u001b[39m.\u001b[39mstart_step_time()\n\u001b[1;32m--> 545\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_join_thread()\n\u001b[0;32m    546\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcurrent_step \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m    547\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mact_buf\u001b[39m.\u001b[39mappend(action)  \u001b[39m# the action is always appended to the buffer\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\rtgym\\envs\\real_time_env.py:389\u001b[0m, in \u001b[0;36mRealTimeEnv._join_thread\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    384\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"This is called at the beginning of every user-side API functions (step(), reset()...) for thread safety.\u001b[39;00m\n\u001b[0;32m    385\u001b[0m \n\u001b[0;32m    386\u001b[0m \u001b[39mThis ensures that the previous time-step is completed when starting a new one\u001b[39;00m\n\u001b[0;32m    387\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    388\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39masync_threading:\n\u001b[1;32m--> 389\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_at_thread\u001b[39m.\u001b[39;49mjoin()\n",
      "File \u001b[1;32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.10_3.10.3056.0_x64__qbz5n2kfra8p0\\lib\\threading.py:1096\u001b[0m, in \u001b[0;36mThread.join\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m   1093\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mcannot join current thread\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m   1095\u001b[0m \u001b[39mif\u001b[39;00m timeout \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m-> 1096\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_wait_for_tstate_lock()\n\u001b[0;32m   1097\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   1098\u001b[0m     \u001b[39m# the behavior of a negative timeout isn't documented, but\u001b[39;00m\n\u001b[0;32m   1099\u001b[0m     \u001b[39m# historically .join(timeout=x) for x<0 has acted as if timeout=0\u001b[39;00m\n\u001b[0;32m   1100\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_wait_for_tstate_lock(timeout\u001b[39m=\u001b[39m\u001b[39mmax\u001b[39m(timeout, \u001b[39m0\u001b[39m))\n",
      "File \u001b[1;32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.10_3.10.3056.0_x64__qbz5n2kfra8p0\\lib\\threading.py:1116\u001b[0m, in \u001b[0;36mThread._wait_for_tstate_lock\u001b[1;34m(self, block, timeout)\u001b[0m\n\u001b[0;32m   1113\u001b[0m     \u001b[39mreturn\u001b[39;00m\n\u001b[0;32m   1115\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m-> 1116\u001b[0m     \u001b[39mif\u001b[39;00m lock\u001b[39m.\u001b[39;49macquire(block, timeout):\n\u001b[0;32m   1117\u001b[0m         lock\u001b[39m.\u001b[39mrelease()\n\u001b[0;32m   1118\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_stop()\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "evaluate_model(agent, env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}